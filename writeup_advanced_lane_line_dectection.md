# writeup: Advanced lane line detection

## 1. Key points of this project:
* camera calibration using chessboard images
* select source and destination points for perspective transformation
* image pre-processing
  * adjust exposure and contrast of image
  * determine the bright zone to filtering the black lines and the lines between light and dark
  * apply Gradient and color space tranformation to select lane pixels
  * warp the image to birds-eye view
  * use histogram to get the lane bases
* fit the lane pixels to second order polynom
  * decide which method to use: sliding_window_poly() or search_around_poly()
  * sliding_window_poly(): use sliding window to search the lane pixels from lane bases
  * search_around_poly(): search the lane pixels around the detected lane lines from last frame
  * fit the found lane pixels to second order polynomials
  * use the polynomials to get the normalized lane points (fixed y-position for all frames)
  * filter the normalized lane points according to the difference to last frame
  * use the filtered lane points to again filter second order polynomials
* compute the lane radius, lane width and the offset to lane middle
* decide if the detected lane lines are valid
  * if the current detection is not valid, then use the last valid detection for image post-processing
  * if the current detection is valid, then save this detection
* image post-processing
  * plot the lane middle and the vehicle middle to represent the offset
  * plot the filtered lane lines as a filled block
  * warp the plotted lane zone back to normal view
  * add the lane zone to the original image

## 2. Camera calibration
The camera calibration has been done by appling the OpenCV function **cv2.calibrateCamera**. For this function two lists have to be computed, "imgpoints" for the detected corners of images for calibration, "objpoints" for the real chess board corners without distortion.

The "objpoints" is a list of numpy-arrays with 6 * 9 rows for each corner and 3 columns for x, y, z coordinates. As the chessboard is assumed to be fixed to the x-y-plane, the z values are 0. The x and y coordinates are generated by the mesh grid function of numpy with transpose and reshape to 2 columns.

The "imgpoints" is a list of numpy-arrays with shape 54 * 1 * 2, which means x- and y-coordinates of the 54 corners on the calibration images, that are successfully detected corners. The corner detection of calibration images uses the OpenCV function **cv.findChessboardCorner()**. Every time the corners are detected, the same coordinate of real chess board will be appended to the "objpoints".

```python
#  camera calibration
images = glob.glob('camera_cal/calibration*.jpg')
imgpoints = []  # list to store image points of all pictures
objpoints = []  # list to store object points of all pictures
#  prepare object points like (0, 0, 0), ..., (5, 8, 0)
objp = np.zeros((54, 3), np.float32)
objp[:, :2] = np.mgrid[0:6, 0:9].T.reshape(-1, 2)
for fname in images:
    img = mpimg.imread(fname)
    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)  # convert to grayscale
    ret, corners = cv2.findChessboardCorners(gray, (6, 9), None)  # find chessboard corners
    if ret == True:
        imgpoints.append(corners)
        objpoints.append(objp)
img = mpimg.imread(images[0])
_, mtx_calc, dist_calc, _, _ = cv2.calibrateCamera(objpoints, imgpoints, img.shape[1::-1], None, None)
```

For undistorting the image a class method **cor_dist** is defined. Here the class **frame_class** defines the methods of the whole processes for lane line detection.

```python
class frame_class(object):
    ...
    ...
        def cor_dist(self, image, mtx=mtx_calc, dist=dist_calc): # distortion Correction
            undst = cv2.undistort(image, mtx, dist, None, mtx)
            return undst
    ...
    ...
```
![Alt text](./output_images/Camera_Calibration.png)

## 3. Perspective Transformation

In this project the perspective transformation uses the OpenCV function **cv2.warpPerspective**. This function needs the transformation matrix, which is computed from source and destination points.

To selecting the source points, a image with straight lane line is used. With the straight lane line is it easier to get a trapezium region, which corresponding to a rectangle region on the warped image.

The source and destination points are defined as follows:

```python
scr = np.float32([[575, 465], [(1280/2-575)+1280/2, 465], [1280/2-264+1280/2, 680], [264, 680]])
dst = np.float32([[350, 100], [640-350+640, 100], [640-350+640, 680], [350, 680]])
```
The source Region on the original image and the destination region on the warped image are as follows:

![Alt text](./output_images/Perspective_Transformation.png)

After defining the source and destination points, the transform matrix and the invert transform matrix should be computed.

```python
M = cv2.getPerspectiveTransform(scr, dst)
Minv = cv2.getPerspectiveTransform(dst, scr)
```

## 4. Image Pre-Processing

The image pre-processing is implemented as a method **img_PrePrc( )** of the class **frame_class**. 

### 4.1 Adjust exposure and contrast of image
In this method the exposure and contrast of the original image will firstly be adjusted, so that the lane lines are clearly apart from the other areas in the image. 
#### 4.1.1 Adjust exposure
To adjusting the exposure the image will be firstly transformed into gray. Then the value of every pixel will be counted and arranged from 0 to 255. The median of the arranged pixel values will then be computed. With assuming that the desired median value is 128, the pixels with its value smaller as 128 will be multiplied with a factor, and the pixels with its value bigger than 128 will be multiplied by different factor, which is in responding to the distance to 255. The goal is, the median to 128 to move.

![Alt text](./output_images/Adjust_Exposure.png)

```python
class frame_class(object):
  ...
  ...    
      def exposure_adj(self, img): # to adjust the exposure of a image. The goal is to adjusting the median of value of its gray image to 128
              gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
              gray_count = gray.ravel() # counting the number of each gray value
              self.gray_count = gray_count
              self.gray_median = np.median(gray_count) # find the median value of the gray image
              if np.abs(self.gray_median - 128) > 15:  # adjust the exposure only when the median far away from 128
                  adj_fac = np.min([128/self.gray_median, 1.5])  # the factor for adjusting is limited to 1.5 times
                  img_out = img.astype(np.float32)
                  img_out[img_out < self.gray_median] = img_out[img_out < self.gray_median] * adj_fac # adjusting the portion, which its value smaller than 128
                  img_out[img_out >= self.gray_median] = ((img_out[img_out >= self.gray_median] - self.gray_median) / (255 - self.gray_median) * (1 - adj_fac) + adj_fac) * img_out[img_out >= self.gray_median] # adjusting the portion, which its value bigger than 128
                  img_out = img_out.astype(np.uint8)
                  return img_out
              else:
                  return img
    ...
    ...
```

#### 4.1.2 Adjust contrast
After getting a image with adjusted exposure, the contrast of this image should be improved, so that the lane line better apart from the ground. In this project the curve function of Photoshop was implemented for fine-tuning the constrast in different areas of the image.

![Alt text](./output_images/Example_Photoshop_Curve.JPG)

 **Example of Photoshop Curve**

Firstly is to consider how to adjust the curve. In this project the curve is used to improve the contrast of highlight zone for detecting white lane lines. As the white lane lines normally have high value in all three channels, by increasing the slope of curve from 160 can increase the contrast of white lane lines to their surrounding areas. 

![Alt text](./output_images/curve_adjusting_for_white_lane_line.png)

The curve is defined in the **init-method** of frame_class and used as factor in the method **ps_curve_proc** and multiplied to the corresponding pixels.

```python
class frame_class(object):  # class, which is to processing every frame
    def __init__(self):
        ...
        self.points = np.array([0, 20, 55, 90, 160, 200, 230, 255])
        self.values = np.array([0, 20, 55, 90, 160, 245, 250, 255])
        self.cs = CubicSpline(points, values)
        ...
    
    def ps_curve_proc(self, input_img, mode='color'): # realize the function of Photoshop curve. In this project it is used to fine tune the range and the transition of brightness and shallow of the image
        if mode == 'color':
          cs = self.cs  # curve for adjusting a color-image
        elif mode == 'gray':
          cs = self.cs_gray # curve for adjusting a gray-image
        else:
          cs = self.cs
        output_img = input_img.copy()
        output_img = cs(output_img)
        output_img[output_img > 255] = 255
        output_img = output_img.astype(np.uint8)
        return output_img
```

![Alt Text](./output_images/result_curve_adjusting.png)

### 4.2 Selecting the bright zone

After improving the exposure and contrast of the image, we have to consider how to avoid detecting the lines between dark and bright zones as lane lines. The image below shows this situation.

![Alt Text](./output_images/Example_Why_Selecting_Bright_zone.png)

The idea to avoid this problem is to firstly selecting the **dark** area. Transforming the data type from logical to 0 and 255 and then applying the GaussianBlur-Function to the selected area, so that the areas, which are bigger than 0, are extended. Now if we made a threshold saying that the pixels between (150,255) belong the selected area before GaussianBlur, than we have by this way extended the dark zone. The problematic lines between dark and bright zones are than filtered.

![Alt Text](./output_images/Shrinking_Bright_zone.png)

```python
class frame_class(object):
    ...
    def img_PrePrc(self):
        ...
        self.bright_zone = (self.color_thresh(self.original_undst_img[:, :, 0], (120, 255))) & (self.color_thresh(self.original_undst_img[:, :, 1], (120, 255))) # select the bright area in the image. Black lines will be filtered
        # shrinking of bright zone to avoid detecting the line, which divides shallow and bright, as lane line
        self.bright_zone_be = 255 * np.int8(np.invert(self.bright_zone))  # inverse bright and shallow
        self.bright_zone_be = cv2.GaussianBlur(self.bright_zone_be, (9, 9), 0) # the bright area (value > 0) which represents the shallow area in the original image, will be extended
        self.bright_zone_be = self.color_thresh(self.bright_zone_be, thresh=(150, 255), mode=1)  # select how much will the bright area be shrinked
        self.bright_zone_be = np.invert(self.bright_zone_be)
        ...
```

### 4.3 Detecting the lane lines depending on color

As the lane lines are yellow or white, these two colors will be selected using color space transformation and thresholding.

The yellow lane lines always have high saturation. To detecting the yellow lane lines, the adjusted image should be firstly turned into HSV-Space. The S-Channel describes the saturation information. Then applying a color-thresholding to the S-Channel. The yellow lane lines are basically detected. (additional conditions in section 4.4)

![Alt Text](./output_images/yellow_lane_line.png)

```python
class frame_class(object):
    ...
    def img_PrePrc(self):
        ...
        self.hsv = cv2.cvtColor(self.cor_dist(self.image), cv2.COLOR_RGB2HSV) # turn the color space into HSV
        self.hsv_s = self.hsv[:, :, 1] # select the S-channel
        self.hsv_s_thresh_y = self.color_thresh(self.hsv_s, thresh=(50, 255), mode=1) # select the yellow lane lines. For yellow lane lines usually have high saturation.
        ...
```

As the white lane lines always have high value of all three RGB-Channels, I used the color thresh to select the pixels with high value (>190) in all three channels to detecting the white lane lines.

![Alt Text](./output_images/white_lane_line.png)

```python
class frame_class(object):
    ...
    def img_PrePrc(self):
        ...
        self.rgb_w = (self.undst_img > [[[190,190,190]]]).all(2) # selecting the white lane lines, for they usually have high values in all three channels
        ...
```

### 4.4 Detecting lane line using gradient and applying all conditions together
 To avoid that some areas, which are in white or yellow but not lane lines, are selected due to the color selection, a gradient threshold has been used together with the yellow and white selection. This, on the other hand, avoids also some areas with high gradient but not yellow or white are selected due gradient thresholding.

![Alt Text](./output_images/Example_detected_lane_lines.png)

```python
class frame_class(object):
    ...
    def img_PrePrc(self):
        ...
        self.grd = self.abs_sobel_thresh(self.gray_img, orient='x', thresh=(10,80)) # detecting the margin of lane lines
        self.sel = (self.rgb_w | self.hsv_s_thresh_y) & self.grd & self.bright_zone_be   # the detected "yellow" and "white" lines are lane lines, only when they are in bright areas while also coresponding margin are detected. the margins are then as detected lane lines for further use.
        ...
```

### 4.5 Perspective Transformation

To classify which pixels belong to lane lines and to compute the parameters of the lane line, the image with detected lane lines should be transformed to the bird-eye view. To do this, the computed transform matrix and the OpenCV Function **cv2.warpPerspective**.

![Alt Text](./output_images/detected_lane_lines_in_bird_eye_view.png)

```python
class frame_class(object):
  ...
  def img_PrePrc(self):
    ...
    self.warped_compute = cv2.warpPerspective(self.sum_compute, M, img_size, flags=cv2.INTER_LINEAR)
    ...
```

### 4.6 Finding the lane lines bases using histogram

For the method sliding_window_poly(), which finds the lane line pixels from beginning, the bases of the lane lines should be firstly be computed. The idea is, using histogram to horizontally count the number of none zero pixels of the bottom half of the image. The peak of the left half should be the left lane line base, the peak on the right should be the right lane line base.

![Alt Text](./output_images/histogram_for_finding_the_lane_line_bases.png)

Furthermore, the found lane line bases can be also used to roughly determine, if the found lane lines are valid. If the lane line base smaller than 2m or bigger than 5m, then it can be as invalid classified. In this project, the frame of invalid found lane lines will be dropped. The last valid frame will be used instead.

```python
 self.histogram = np.sum(self.warped[self.warped.shape[0] // 2:, :], axis=0)  # computing the histogram
        midpoint = np.int(self.histogram.shape[0] // 2)
        self.left_base = np.argmax(self.histogram[:midpoint]) # finding the bases of the left and right lane lines
        self.right_base = np.argmax(self.histogram[midpoint:]) + midpoint
        if (self.right_base - self.left_base) * self.xm_per_pix < 2 or (
                self.right_base - self.left_base) * self.xm_per_pix > 5: # preliminarily consider if the detected lane lines are useful or not
            self.frame_drop = True  # this frame will be dropped from detecting lane lines
            # pass
        else:
            self.frame_drop = False
```

## 5. Fit the lane pixels to second order polynom

### 5.1 Finding lane pixels using sliding window

As in the course introduced, in this project the sliding window is used to finding the lane pixels as a basic or fallback method.

```python
    def find_lane_pixel(self):  # finding lane pixels using sliding window from lane line bases
        nwindows = 9
        margin = 80
        minpix = 100

        window_height = np.int(self.warped_compute.shape[0] // nwindows)

        nonzero = self.warped_compute.nonzero()
        nonzeroy = nonzero[0]
        nonzerox = nonzero[1]

        left_loc = self.left_base
        right_loc = self.right_base

        left_lane_idx = []
        right_lane_idx = []

        for window in range(nwindows):
            win_y_low = self.warped_compute.shape[0] - (window + 1) * window_height
            win_y_high = self.warped_compute.shape[0] - window * window_height
            win_left_low = left_loc - margin
            win_left_high = left_loc + margin
            win_right_low = right_loc - margin
            win_right_high = right_loc + margin

            # cv2.rectangle(warped_compute, (win_left_low, win_y_low), (win_left_high, win_y_high), (255), 2)
            # cv2.rectangle(warped_compute, (win_right_low, win_y_low), (win_right_high, win_y_high), (255), 2)

            good_left_idx = ((nonzeroy > win_y_low) & (nonzeroy < win_y_high) & (nonzerox > win_left_low) & (
                        nonzerox < win_left_high)).nonzero()[0]
            good_right_idx = ((nonzeroy > win_y_low) & (nonzeroy < win_y_high) & (nonzerox > win_right_low) & (
                        nonzerox < win_right_high)).nonzero()[0]

            left_lane_idx.append(good_left_idx)
            right_lane_idx.append(good_right_idx)

            if len(good_left_idx) > minpix:
                left_loc = np.int(np.mean(nonzerox[good_left_idx]))
            if len(good_right_idx) > minpix:
                right_loc = np.int(np.mean(nonzerox[good_right_idx]))

        left_lane_idx = np.concatenate(left_lane_idx)
        right_lane_idx = np.concatenate(right_lane_idx)

        self.leftx = nonzerox[left_lane_idx]
        self.lefty = nonzeroy[left_lane_idx]
        self.rightx = nonzerox[right_lane_idx]
        self.righty = nonzeroy[right_lane_idx]
```

After finding the lane pixels, it should be determined, if the found lane line pixels able to be fitted as second order polynom.

```python
        if self.lefty.shape[0] < 3 or self.righty.shape[0] < 3: # if the detected lane pixel less than three, then drop this frame from detecting lane lines.
            self.frame_drop = True
            pass
```

Then the Numpy-Function **np.polyfit** can be used to fit the found lane line pixels into a second order polynom

````python
self.left_fit = np.polyfit(self.lefty, self.leftx, 2) # fit the pixels as lane line using second order polynom
self.right_fit = np.polyfit(self.righty, self.rightx, 2)
````

In this project, to filtering between two frames, a low pass filter was used. The time constant of the low pass filter depends on the difference of two frames. The bigger the difference, the larger the time constant and so the filtering effect. To varify the lane lines between two frames, the lane lines were normalized using points with fixed vertical position. The difference of these points in horizontal will be filtered. Besides, the normalized lane line can also be used to test the parallelism of left and right lane lines.

```python
# using the fitted polynom for the normalized lane line with fixed y-position. This will be used for filtering between frames
self.ploty = np.linspace(0, self.warped_compute.shape[0] - 1, self.warped_compute.shape[0])
self.left_fitx = self.left_fit[0] * self.ploty ** 2 + self.left_fit[1] * self.ploty + self.left_fit[2]
self.right_fitx = self.right_fit[0] * self.ploty ** 2 + self.right_fit[1] * self.ploty + self.right_fit[2]


if np.abs(np.max((self.right_fitx - self.left_fitx) * self.xm_per_pix) - np.min((self.right_fitx - self.left_fitx) * self.xm_per_pix)) > 4 or (self.right_fitx < self.left_fitx).any():  # judging if the detected lane lines are valid: 1. if the widths between the widest and the narrowest lane lines greater than 4m, then drop this frame. 2. if the detected left lane line on the right of the detected right lane line, then drop this frame.
  self.frame_drop = True
  pass
```

Filter lane lines:

```python
def lane_line_filter(self):  # filtering the lane lines between frames using normalized lane points
    tiFil_t = [[10, 20, 50, 100, 120, 200], [0.04, 0.05, 0.08, 0.1, 0.8, 1]]  # tiFil_t[0]: difference between two frames in pixel, tiFil_t[1]: time constant for a low pass filter
    if (self.counter <= 1) or self.left_fitx_mem == []: # initialize
        self.left_fitx_mem = self.left_fitx.copy()
        self.right_fitx_mem = self.right_fitx.copy()
    else:
        delta_left = np.abs(self.left_fitx_mem - self.left_fitx)  # compute the difference between two frames of each normalized lane points
        # delta_left_max = delta_left.max()
        # tiFil_left = self.getZValueFromTabel(delta_left_max, tiFil_t) # using the biggest difference to get the time constant
        tiFil_left = self.getZValueFromTabel(delta_left, tiFil_t)  # using the biggest difference to get the time constant

        delta_right = np.abs(self.right_fitx_mem - self.right_fitx)
        # delta_right_max = delta_right.max()
        # tiFil_right = self.getZValueFromTabel(delta_right_max, tiFil_t)
        tiFil_right = self.getZValueFromTabel(delta_right, tiFil_t)

        self.left_fitx_mem = self.left_fitx_mem + 0.02 / tiFil_left * (self.left_fitx - self.left_fitx_mem) # compute the x-position of filtered lane points
        self.right_fitx_mem = self.right_fitx_mem + 0.02 / tiFil_right * (self.right_fitx - self.right_fitx_mem)

    self.left_fitx = self.left_fitx_mem.copy()
    self.right_fitx = self.right_fitx_mem.copy()
```

The function **getZValueFromTabel** is used to read the time constant from a lookup tabel:

```python
def getZValueFromTabel(self, input_x, tiFil_tabel): # _vectorized
  i = 0
  output_ti = np.ones_like(input_x) * tiFil_tabel[1][0]  # all outputs are initiallized with the first value in the tabel
  while i < len(tiFil_tabel[0]) - 1:
    output_ti[input_x > tiFil_tabel[0][i]] = tiFil_tabel[1][i + 1]
    i += 1
  return output_ti
```

The filtered "lane points" should be once again be fitted to second order polynom and then again normalized using points with fixed vertical position.

```python
self.left_fit = np.polyfit(self.ploty, self.left_fitx, 2)  # fitting second order polynom after filtering of the lane line
self.right_fit = np.polyfit(self.ploty, self.right_fitx, 2)
self.ploty = np.linspace(0, self.warped_compute.shape[0] - 1, self.warped_compute.shape[0])
self.left_fitx = self.left_fit[0] * self.ploty ** 2 + self.left_fit[1] * self.ploty + self.left_fit[2]
self.right_fitx = self.right_fit[0] * self.ploty ** 2 + self.right_fit[1] * self.ploty + self.right_fit[2]
```

d

d

d

d

d